{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class nn:\n",
    "    def __init__(self,hidden_layer_size=100,learning_rate=0.01,neurons=20,iterations=60000,epsilon=1e5,activation_function='tan'):\n",
    "        self.hidden_layer_size=hidden_layer_size\n",
    "        self.activation_function=activation_function\n",
    "        self.learning_rate=learning_rate\n",
    "        self.layer=list()\n",
    "        self.layer_weights=list()\n",
    "        self.output_layer=1\n",
    "        self.iterations=iterations\n",
    "        self.epsilon=epsilon\n",
    "        self.neurons=neurons\n",
    "        \n",
    "        \n",
    "    def create_network(self,X):\n",
    "        #np.random.seed(1) #to have random in between the specific range\n",
    "        random_weights=2*np.random.random((X.shape[1],self.neurons))-1\n",
    "        self.layer_weights.append(random_weights)\n",
    "        for i in range(self.hidden_layer_size-2):\n",
    "            random_weights=2*np.random.random((self.neurons,self.neurons))-1\n",
    "            self.layer_weights.append(random_weights)\n",
    "        random_weights=2*np.random.random((self.neurons,self.output_layer))-1\n",
    "        self.layer_weights.append(random_weights)\n",
    "        \n",
    "        \n",
    "    def activation(self,x,derivative=False):\n",
    "        if derivative:\n",
    "            if self.activation_function == \"sigmoid\":\n",
    "                return x * (1 - x)\n",
    "            if self.activation_function==\"tan\":\n",
    "                return 1.0 - np.tanh(x)**2\n",
    "            if self.activation_function == \"ReLU\":\n",
    "                return (x > 0).astype(int)        \n",
    "        else:\n",
    "            if self.activation_function == \"sigmoid\":\n",
    "                return 1 / (1 + np.exp(-x))\n",
    "            if self.activation_function==\"tan\":\n",
    "                    return np.tanh(x)\n",
    "            if self.activation_function == \"ReLU\":\n",
    "                return x * (x > 0)\n",
    "            \n",
    "        \n",
    "    def fit(self,X,Y):\n",
    "        end_error=0\n",
    "        self.create_network(X)\n",
    "        for _ in range(self.iterations):\n",
    "            #feed forward throught the network\n",
    "            self.layer=list()\n",
    "            self.layer.append(X)\n",
    "            for i in range(self.hidden_layer_size):\n",
    "                hidden_layer=self.activation(np.dot(self.layer[i],self.layer_weights[i]))\n",
    "                self.layer.append(hidden_layer)\n",
    "            \n",
    "            error=Y-self.layer[-1]\n",
    "            end_error=np.mean(np.abs(error))\n",
    "            if(_%100==1):\n",
    "                print(str(_)+\" Error \"+str(end_error))\n",
    "            for i in range(self.hidden_layer_size,0,-1):\n",
    "                delta = error*self.activation(self.layer[i],derivative=True)\n",
    "                error = delta.dot(self.layer_weights[i-1].T)\n",
    "                self.layer_weights[i-1] += self.layer[i-1].T.dot(delta)\n",
    "       \n",
    "        print(\"End Error\"+str(end_error))\n",
    "\n",
    "    \n",
    "    def predict(self,X):\n",
    "        predicted=X\n",
    "        for i in range(self.hidden_layer_size):\n",
    "            predicted=self.activation(np.dot(predicted,self.layer_weights[i]))\n",
    "        return predicted.ravel()\n",
    "    \n",
    "    #need to remove\n",
    "    def sigmoid(self,x,derivative):\n",
    "            if(derivative==1):\n",
    "                return 1.0 - np.tanh(x)**2\n",
    "            return np.tanh(x)\n",
    "        \n",
    "    \n",
    "    def layer3(self,X,Y,X_test):\n",
    "        y=Y\n",
    "        phi=X\n",
    "        rows,col=phi.shape\n",
    "\n",
    "        num_of_input_neurons=X.shape[1]\n",
    "        num_of_hidden_layer1_neurons=self.neurons\n",
    "        num_of_hidden_layer2_neurons=self.neurons\n",
    "        num_of_output_layer_neurons=1\n",
    "        \n",
    "        wt1=2*np.random.random((num_of_input_neurons,num_of_hidden_layer1_neurons))-1\n",
    "        wt2=2*np.random.random((num_of_hidden_layer1_neurons,num_of_hidden_layer2_neurons))-1\n",
    "        wt3=2*np.random.random((num_of_hidden_layer2_neurons,num_of_output_layer_neurons))-1\n",
    "        \n",
    "        data=phi\n",
    "        train_y=np.reshape(y,(rows,1))\n",
    "        train_x= np.array(data)\n",
    "        \n",
    "        for j in range(self.iterations):\n",
    "            layer1_output=self.sigmoid(np.dot(train_x,wt1),0)\n",
    "            layer2_output=self.sigmoid(np.dot(layer1_output,wt2),0)\n",
    "            layer3_output=self.sigmoid(np.dot(layer2_output,wt3),0)\n",
    "            layer3_error=layer3_output-train_y\n",
    "            layer3_senstivity = layer3_error*self.sigmoid(layer3_output,1)\n",
    "            layer2_error=layer3_senstivity.dot(wt3.T)\n",
    "            layer2_senstivity=layer2_error*(self.sigmoid(layer2_output,1))\n",
    "            layer1_error=layer2_senstivity.dot(wt2.T)\n",
    "            layer1_senstivity=layer1_error*(self.sigmoid(layer1_output,1))\n",
    "            wt3=wt3-(self.learning_rate*layer2_output.T.dot(layer3_senstivity))\n",
    "            wt2=wt2-self.learning_rate*layer1_output.T.dot(layer2_senstivity)\n",
    "            wt1=wt1-self.learning_rate*(train_x.T).dot(layer1_senstivity)\n",
    "\n",
    "        layer1= self.sigmoid(np.dot(X_test,wt1),0)\n",
    "        layer2= self.sigmoid(np.dot(layer1,wt2),0)\n",
    "        layer3= self.sigmoid(np.dot(layer2,wt3),0)\n",
    "        layer3=layer3.ravel()\n",
    "        layer3[layer3 > 0]=1\n",
    "        layer3[layer3<=0]=0\n",
    "        return layer3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize(inputData):\n",
    "    #return (inputData - inputData.mean()) / inputData.std()\n",
    "    return (inputData - inputData.min()) / (inputData.max() - inputData.min())\n",
    "\n",
    "#reads the datafiles and returns the training and the testing data\n",
    "def get_data():\n",
    "    # get test & test csv files as a DataFrame\n",
    "    train_df = pd.read_csv(\"data/train.csv\")\n",
    "    test_df    = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "    \n",
    "    #removing race and native country\n",
    "    train_df=train_df.drop(['race'],axis=1)\n",
    "    train_df=train_df.drop(['native-country'],axis=1)\n",
    "#     train_df=train_df.drop(['occupation'],axis=1)\n",
    "    train_df=train_df.drop(['relationship'],axis=1)\n",
    "\n",
    "    test_df=test_df.drop(['race'],axis=1)\n",
    "    test_df=test_df.drop(['native-country'],axis=1)\n",
    "#     test_df=test_df.drop(['occupation'],axis=1)\n",
    "    test_df=test_df.drop(['relationship'],axis=1)\n",
    "\n",
    "    numericalColumns = ('age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week')\n",
    "    for i in numericalColumns:\n",
    "        train_df[i] = normalize(train_df[i])\n",
    "        test_df[i] = normalize(test_df[i])\n",
    "\n",
    "    \n",
    "    #creating dummies of the data\n",
    "    train_df=pd.get_dummies(train_df)\n",
    "    test_df=pd.get_dummies(test_df)\n",
    "\n",
    "    #remove unwanted columns and the columns that are created for ?\n",
    "    columns_to_remove=set(list(train_df)).symmetric_difference(set(list(test_df)))\n",
    "    columns_to_remove.remove('salary')\n",
    "    for col in list(train_df):\n",
    "        if (col in columns_to_remove) or (\"?\" in col) :\n",
    "            print(col)\n",
    "            train_df=train_df.drop(col,1)\n",
    "    for col in list(test_df):\n",
    "        if (col in columns_to_remove) or (\"?\" in col) :\n",
    "            test_df=test_df.drop(col,1)\n",
    "    \n",
    "    return train_df,test_df\n",
    "\n",
    "\n",
    "def process_data(percent):\n",
    "    train_df,test_df=get_data()\n",
    "    test_ids=test_df['id'].as_matrix()\n",
    "    train_df=train_df.drop(['id'],1)\n",
    "    test_df=test_df.drop(['id'],1)\n",
    "    train_df['const']=1\n",
    "    test_df['const']=1\n",
    "    Y=train_df['salary'].as_matrix()\n",
    "    X=train_df.drop(['salary'], axis=1).as_matrix()\n",
    "    Y=Y.reshape(len(Y),1)\n",
    "    end=int(X.shape[0] * percent)\n",
    "    #training data\n",
    "    train_X=X[:end,:]\n",
    "    train_Y=Y[:end,:]\n",
    "    #data for cross validation\n",
    "    cross_X=X[end:,:]\n",
    "    cross_Y=Y[end:,:]\n",
    "    #testing data\n",
    "    test_X=test_df.as_matrix()\n",
    "    \n",
    "    return train_X,train_Y,cross_X,cross_Y,test_X,test_ids\n",
    "\n",
    "\n",
    "\n",
    "#writes the predicted values to file \n",
    "def write_result(ids,predicted,file_name):\n",
    "    output=np.column_stack((ids,predicted))\n",
    "    np.savetxt(file_name,output,delimiter=\",\",fmt=\"%d,%d\",header=\"id,salary\",comments ='')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workclass_ ?\n",
      "occupation_ ?\n",
      "1 Error 0.762906702248\n",
      "101 Error 0.762906702248\n",
      "201 Error 1.23709329775\n",
      "301 Error 0.777686544186\n",
      "401 Error 0.764035717951\n",
      "501 Error 0.762906702248\n",
      "601 Error 0.79636662219\n",
      "701 Error 1.14369290773\n",
      "801 Error 0.992917992405\n",
      "901 Error 1.18628759109\n",
      "1001 Error 0.770296623217\n",
      "1101 Error 1.23709329775\n",
      "1201 Error 0.763830442369\n",
      "1301 Error 1.23709329775\n",
      "1401 Error 1.20691778713\n",
      "1501 Error 1.01324023865\n",
      "1601 Error 0.988607204029\n",
      "1701 Error 0.762906702248\n",
      "1801 Error 0.763009340039\n",
      "1901 Error 1.23709329775\n",
      "2001 Error 1.22395566047\n",
      "2101 Error 0.763419891204\n",
      "2201 Error 1.23709329775\n",
      "2301 Error 0.767114851688\n",
      "2401 Error 1.19511444114\n",
      "2501 Error 0.767730678436\n",
      "2601 Error 1.23709329775\n",
      "2701 Error 0.97505901673\n",
      "2801 Error 1.18875089808\n",
      "2901 Error 0.764343631325\n",
      "3001 Error 0.762906702248\n",
      "3101 Error 0.762906702248\n",
      "3201 Error 0.891717130247\n",
      "3301 Error 1.21656573951\n",
      "3401 Error 0.767525402853\n",
      "3501 Error 0.764035717951\n",
      "3601 Error 0.771630914503\n",
      "3701 Error 1.22611105409\n",
      "3801 Error 1.23452735297\n",
      "3901 Error 0.763317253413\n",
      "4001 Error 0.76639638715\n",
      "4101 Error 1.23339833727\n",
      "4201 Error 1.23103766807\n",
      "4301 Error 1.22590577851\n",
      "4401 Error 0.764651544699\n",
      "4501 Error 0.768962331931\n",
      "4601 Error 0.768962331931\n",
      "4701 Error 0.764959458072\n",
      "4801 Error 1.22857436108\n",
      "4901 Error 0.764240993534\n",
      "End Error1.23370625064\n",
      "(6878,)\n",
      "(6878,) (6878,)\n"
     ]
    }
   ],
   "source": [
    "train_X,train_Y,cross_X,cross_Y,test_X,test_ids= process_data(0.50)\n",
    "neural_network=nn(hidden_layer_size=3,neurons=30,iterations=5000,learning_rate=0.0001)\n",
    "neural_network.fit(train_X,train_Y)\n",
    "predict=neural_network.predict(test_X)\n",
    "print(predict.shape)\n",
    "predict[predict>0]=1\n",
    "predict[predict<=0]=0\n",
    "print(test_ids.shape,predict.shape)\n",
    "write_result(test_ids,predict,\"hareesh.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t=neural_network.layer3(train_X,train_Y,test_X)\n",
    "write_result(test_ids,t,\"ayush.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6878,) (6878,)\n"
     ]
    }
   ],
   "source": [
    "predict[predict>0]=1\n",
    "predict[predict<=0]=0\n",
    "print(test_ids.shape,predict.shape)\n",
    "write_result(test_ids,predict,\"hareesh.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#playgorund\n",
    "X= np.array([ [0,0,1],\n",
    "              [0,1,1],\n",
    "              [1,0,1],\n",
    "              [1,1,1], ])\n",
    "\n",
    "Y=np.array([ [ 0,1,1,1 ] ]).T\n",
    "\n",
    "n=nn(hidden_layer_size=1,neurons=20)\n",
    "\n",
    "predict=n.ayush(X,Y,X)\n",
    "print(predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
